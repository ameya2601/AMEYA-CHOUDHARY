#Let's consider a Company dataset with around 10 variables and 400 records. 
#A cloth manufacturing company is interested to know about the segment or attributes causes high sale. 
#Approach - A Random Forest can be built with target variable Sales (we will first convert it in categorical variable) & all other variable will be independent in the analysis.  


install.packages("MASS")
install.packages ("randomForest")

library(randomForest)
library(MASS)
library(caret)

CompData <- read.csv(file.choose())

hist(CompData$Sales, main = "Sales of Companydata",xlim = c(0,20),
     breaks=c(seq(10,20,30)), col = c("blue","red", "green","violet"))

highsales = ifelse(CompData$Sales<9, "No", "Yes")  # if greater than 8 then high sales else Low

CD = data.frame(CompData[2:11], highsales)
str(CD)

table(CD$highsales)   # 114 company have high sales and 286 have low sales

# Data Partition

set.seed(123)
ind <- sample(2, nrow(CD), replace = TRUE, prob = c(0.7,0.3))
train <- CD[ind==1,]
test  <- CD[ind==2,]
set.seed(213)

#random forest analysis

rf <- randomForest(highsales~., data=train)
rf  # Description of the random forest with no of trees, mtry = no of variables for splitting

# each tree node.
# Out of bag estimate of error rate is 18.25 % in Random Forest Model.

# Prediction and Confusion Matrix - Training data 
pred1 <- predict(rf, train)
head(pred1)

head(train$highsales)

#the first six predicted values exactly matches the orginal values

confusionMatrix(pred1, train$highsales)

# 100 % accuracy on training data 

# more than 95% Confidence Interval. 
# Sensitivity for Yes and No is 100 % 

#**********
  
  # Prediction with test data - Test Data 
  
  pred2 <- predict(rf, test)
confusionMatrix(pred2, test$highsales) # 84.35 % accuracy on test data 

#error rate in random forest:

plot(rf)

#tune random forest.

tune <- tuneRF(train[,-11], train[,11], stepFactor = 0.5, plot = TRUE, ntreeTry = 300,
               trace = TRUE, improve = 0.05)

#random forest model for train dataset

rf1 <- randomForest(highsales~., data=train, ntree = 300, mtry = 3, importance = TRUE,
                    proximity = TRUE)
rf1   #OOB estimate of  error rate: 17.89%


pred1 <- predict(rf1, train)
confusionMatrix(pred1, train$highsales)  # 100 % accuracy on training data 


# test data prediction using the Tuned RF1 model

pred2 <- predict(rf1, test)
confusionMatrix(pred2, test$highsales) # 83.48 % accuracy on test data 
# Confidence Interval is around 90 % 

# no of nodes of trees

hist(treesize(rf1), main = "No of Nodes for the trees", col = "green")

# Majority of the trees has an average number of 45 to 50 nodes. 

# Variable Importance :

varImpPlot(rf1)

varImpPlot(rf1 ,Sort = T, n.var = 5, main = "Top 5 -Variable Importance")

# Quantitative values 
importance(rf1)

varUsed(rf)   # which predictor variables are actually used in the random forest.

# Partial Dependence Plot 
partialPlot(rf1, train, Price, "Yes")

# On that graph, if the price is 100 or greater, than they are not buying those clothes.

# Extract single tree from the forest :

getTree(rf, 1, labelVar = TRUE)

# Multi Dimension scaling plot of proximity Matrix
MDSplot(rf1, CompData$highsales)



#2. Use Random Forest to prepare a model on fraud data 
treating those who have taxable_income <= 30000 as "Risky" and others are "Good"


library(randomForest)
library(MASS)
library(caret)

set.seed(123)

FraudCheck <- read.csv(file.choose())

hist(FraudCheck$Taxable.Income)

hist(FraudCheck$Taxable.Income, main = "Sales of Companydata",xlim = c(0,100000),
     breaks=c(seq(40,60,80)), col = c("blue","red", "green","violet"))

Risky_Good = ifelse(FraudCheck$Taxable.Income<= 30000, "Risky", "Good")

# if Taxable Income is less than or equal to 30000 then Risky else Good.

FCtemp= data.frame(FraudCheck,Risky_Good)
FC = FCtemp[,c(1:7)]

str(FC)

table(FC$Risky_Good)  # 476 good customers and 124 risky customers

# Data Partition
set.seed(123)
ind <- sample(2, nrow(FC), replace = TRUE, prob = c(0.7,0.3))
train <- FC[ind==1,]
test  <- FC[ind==2,]
set.seed(213)
rf <- randomForest(Risky_Good~., data=train)
rf  

# Description of the random forest with no of trees, mtry = no of variables for splitting

# each tree node.
# Out of bag estimate of error rate is 0.47 % in Random Forest Model.
attributes(rf)

# Prediction and Confusion Matrix - Training data 
pred1 <- predict(rf, train)
head(pred1)

head(train$Risky_Good)

# The first six predicted value and original value matches.

confusionMatrix(pred1, train$Risky_Good)  #100 % accuracy on training data, more than 99% Confidence Interval. 

# Prediction with test data - Test Data 
pred2 <- predict(rf, test)
confusionMatrix(pred2, test$Risky_Good) # 100 % accuracy on test data, more than 97% Confidence Interval.  

# Error Rate in Random Forest Model :
plot(rf) ## at 200 there is a constant line and it doesnot vary after 200 trees


# Tune Random Forest Model mtry 
tune <- tuneRF(train[,-6], train[,6], stepFactor = 0.5, plot = TRUE, ntreeTry = 300,
               trace = TRUE, improve = 0.05)

rf1 <- randomForest(Risky_Good~., data=train, ntree = 200, mtry = 2, importance = TRUE,
                    proximity = TRUE)
rf1

#prediction analysis on random forest model with train data set

pred_rf1 <- predict(rf1, train)
confusionMatrix(pred_rf1, train$Risky_Good)  #100 % accuracy on training data, # Around 99% Confidence Interval.

# prediction using the Tuned RF1 model

pred_rf2 <- predict(rf1, test)
confusionMatrix(pred_rf2, test$Risky_Good) # 100 % accuracy on test data, Around 97% Confidence Interval. 

# no of nodes of trees
hist(treesize(rf1), main = "No of Nodes for the trees", col = "green")

# Majority of the trees has an average number of more than 80 nodes. 

# Variable Importance :
varImpPlot(rf1)

varImpPlot(rf1 ,Sort = T, n.var = 5, main = "Top 5 -Variable Importance")

importance(rf1)

varUsed(rf) # which predictor variables are actually used in the random forest.


# Partial Dependence Plot 
partialPlot(rf1, train, Taxable.Income, "Good")

# On the graph, If the taxable Income is 30000 or greater,
# than they are good customers else those are risky customers.
# Extracting single tree from the forest :

tr1 <- getTree(rf1, 2, labelVar = TRUE)

# Multi Dimension scaling plot of proximity Matrix
MDSplot(rf1, FC$Risky_Good)

